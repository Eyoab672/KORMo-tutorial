# Commons
vocab_size: 125184
max_position_embeddings: 131072
initializer_range: 0.02
rms_norm_eps: 0.00001
mlp_bias: False
attention_bias: False
tie_word_embeddins: False
pad_token_id: null
bos_token_id: 125030
eos_token_id: 125031
pretrain_tp: 1
rope_theta: 500000.0
attention_dropout: 0.0
_attn_implementation: "flash_attention_2"
dtype: "bfloat16"

hidden_size: 4096
intermediate_size: 16384
num_hidden_layers: 40
num_attention_heads: 32
num_key_value_heads: 8
head_dim: 128