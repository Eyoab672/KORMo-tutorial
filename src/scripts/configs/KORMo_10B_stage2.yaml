data:
  dataset_name: "hard coding"

model:
  model_size: 10B
  _attn_implementation: flash_attention_3_doc
  tokenizer_name_or_path: kormo-lm/KORMo-tokenizer

train:
  output_dir: "/fsx/KORMo/saved_models/KORMo-10B-stage2_theta500k"
  num_train_epochs: 1
  save_steps: 1000
  bf16: true
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  warmup_ratio: 0.03
  ddp_find_unused_parameters: false
  gradient_checkpointing: false
  learning_rate: 0.0007
  lr_scheduler_type: "warmup_stable_decay"

  optim: adamw_torch
  weight_decay: 0.033
  report_to: wandb
  run_name: KORMo-10B-stage2-500k