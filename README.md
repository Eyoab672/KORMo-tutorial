# KORMo-tutorial

## [ðŸ¤— Model and Dataset](https://huggingface.co/KORMo-Team)
## [ðŸ“„ Paper](https://arxiv.org/abs/2510.09426)

This repository provides tutorial materials for **KORMo(Korean Open Reasoning Model)**, a Korean Large Language Model (LLM) project built with the Hugging Face ecosystem.  
It demonstrates how to **pretrain**, **fine-tune**, and **evaluate** large-scale language models using modern open-source frameworks.

---

### ðŸ§© Setup Environment
```bash
bash setup/create_uv_venv.sh
```

This script creates an isolated virtual environment and installs all dependencies required to run the tutorials.

### ðŸ“˜ Tutorials Included

You can find step-by-step examples in the `tutorial` directory:
```graphql
tutorial
  â”œâ”€â”€ 01.pretrain_from_scratch.ipynb     # Pretraining a language model from scratch using custom data
  â”œâ”€â”€ 02.sft_qlora.ipynb                 # Supervised Fine-Tuning with QLoRA for efficiency
  â””â”€â”€ 03.inference.ipynb                 # Performing inference and evaluating the trained model
```

Each notebook is designed to be self-contained and runnable within the prepared environment.

### ðŸš€ Overview

These tutorials aim to help researchers and practitioners:

- Understand the full training pipeline of large Korean language models
- Learn how to use Hugging Face Transformers, Datasets, and PEFT (Parameter-Efficient Fine-Tuning)
- Experiment with QLoRA and distributed training setups
- Run inference and evaluation on trained checkpoints

### ðŸ§  Credits 
Developed by the KORMo Team.
