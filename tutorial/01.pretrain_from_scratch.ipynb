{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276af8b0",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/MLP-Lab/KORMo-tutorial.git\n",
    "!cd KORMo-tutorial & bash setup/create_uv_venv.sh\n",
    "!source .venv_kormo/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c15283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "sys.path.append('/content/KORMo-tutorial/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d94be",
   "metadata": {},
   "source": [
    "# 1. Preparing the Dataset for Pretraining\n",
    "\n",
    "This section builds the dataset pipeline for language model pretraining.\n",
    "\n",
    "- 1. Load the raw dataset\n",
    "- 2. Tokenize the text\n",
    "- 3. Pack sequences\n",
    "- 4. Data collator\n",
    "- 5. Verify the processed data with a DataLoader\n",
    "\n",
    "### Raw text $\\to$ Tokenize $\\to$ Sequence packing $\\to$ Data Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071f473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 16000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the raw dataset from 'KORMo-Team/KORMo-tutorial-datasets'.\n",
    "import datasets \n",
    "\n",
    "dataset_repo_id = 'KORMo-Team/KORMo-tutorial-datasets'\n",
    "\n",
    "pt_dataset = datasets.load_dataset(\n",
    "    dataset_repo_id, \n",
    "    name='pretrain', \n",
    "    split='train'\n",
    ")\n",
    "\n",
    "train_ds = pt_dataset.shuffle(seed=42)\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150b4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "tokenizer_repo_id = 'KORMo-Team/KORMo-tokenizer'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1124561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 16000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "\n",
    "def _tokenize(examples, tokenizer):\n",
    "    input_ids = []\n",
    "    for text in examples['text']:\n",
    "        input_ids.append(tokenizer.encode(text) + [tokenizer.eos_token_id])\n",
    "    return{\n",
    "        'input_ids': input_ids\n",
    "    }\n",
    "\n",
    "tokenized_ds = train_ds.map(\n",
    "    _tokenize, \n",
    "    batched=True, \n",
    "    num_proc=48,\n",
    "    remove_columns=train_ds.column_names,\n",
    "    fn_kwargs={'tokenizer': tokenizer},\n",
    ")\n",
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97bf5c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|BOS|> Abstract Reasoning in Business and Management: An In-Depth Analysis\n",
      "\n",
      "Introduction\n",
      "\n",
      "In the world of business and management, abstract reasoning plays a crucial role in making informed decisions, solving complex problems, and navigating the ever-changing landscape of the corporate world. It involves the ability to identify patterns, logical rules, and structures underlying different situations, independent of concrete experience with those situations. This skillset enables managers and leaders to think strategically, analyze data effectively, and adapt to new challenges with ease. In this chapter, we delve deep into the concept of abstract reasoning, its significance in business and management, and provide practical examples to illustrate its applications.\n",
      "\n",
      "Understanding Abstract Reasoning\n",
      "\n",
      "At its core, abstract reasoning refers to the cognitive process of recognizing and manipulating abstract patterns, relationships, and structures. Unlike other forms of thinking, which often rely on concrete experiences or familiar scenarios, abstract reasoning operates independently of any particular content domain. Instead, it focuses on identifying commonalities across various domains and discerning the underlying principles governing them (Halpern, 2014).\n",
      "\n",
      "The Role of Abstract Reasoning in Business and Management\n",
      "\n",
      "In today's fast-paced and interconnected global economy, businesses face increasingly complex challenges requiring sophisticated problem-solving skills and strategic decision-making abilities. Abstract reasoning emerges as a vital competence for professionals working in diverse fields such as finance, marketing, human resources, operations management, and entrepreneurship. By cultivating their capacity for abstraction, these individuals enhance their ability to:\n",
      "\n",
      "1. Identify Patterns and Trends: Professionals adept at abstract reasoning can quickly spot emerging trends, market shifts, and competitive dynamics by analyzing vast amounts of seemingly disparate information. For instance, financial analysts may utilize abstract reasoning techniques to recognize cyclical patterns in stock prices or economic indicators, informing investment strategies and risk management practices.\n",
      "2. Develop Strategic Plans: Senior executives must frequently formulate long-term plans encompassing multiple variables and contingencies. Abstract reasoning allows them to envision possible future states, assess alternative courses of action, and anticipate potential obstacles. Consequently, they can devise more robust and resilient strategies capable of adapting to unforeseen circumstances.\n",
      "3. Solve Complex Problems: Managers routinely encounter intricate issues demanding creative solutions grounded in solid analytical foundations. Abstract reasoning facilitates problem identification, definition, and resolution by enabling professionals to dissect complex problems into simpler components, isolate key variables, and generate innovative alternatives.\n",
      "4. Communicate Effectively: Strong abstract reasoning skills also contribute to effective communication within organizations. By distilling complex ideas into digestible concepts and presenting them coherently, managers foster better collaboration, knowledge sharing, and alignment among team members.\n",
      "5. Learn Continuously: Finally, abstract reasoning fosters lifelong learning by promoting transferable thinking skills applicable across various disciplines and industries. As professionals expand their knowledge base, strong abstract reasoning capabilities enable them to integrate new information seamlessly, synthesize novel insights, and apply lessons learned from one context to another.\n",
      "\n",
      "Practical Applications of Abstract Reasoning in Business and Management\n",
      "\n",
      "To fully appreciate the value of abstract reasoning in business and management, let us explore some practical applications through real-world examples:\n",
      "\n",
      "1. Scenario Planning: Shell Oil Company has been renowned for its pioneering work in scenario planningâ€”a technique harnessing abstract reasoning to envision plausible futures and prepare for potential outcomes. By examining macrotrends, drivers of change, and underlying assumptions, Shell's strategists construct compelling narratives around alternate worlds, thereby enhancing organizational agility and responsiveness (Schwartz, 1996).\n",
      "2. Data Analytics: In the era of big data, companies like Amazon and Netflix leverage advanced analytics tools to mine customer preferences, behavioral patterns, and consumption habits. Through rigorous abstraction and pattern recognition, these firms tailor personalized recommendations, optimize supply chain management, and predict demand fluctuations, ultimately driving growth and profitability (Kiron et al., 2014).\n",
      "3. Design Thinking: Rooted in abstract reasoning, design thinking offers a structured approach to innovation that emphasizes empathy, experimentation, and iterative refinement. By abstracting user needs, desires, and pain points, practitioners create elegant solutions addressing latent demands while minimizing risks associated with traditional product development processes (Brown & Katz, 2011).\n",
      "4. Change Management: When implementing large-scale transformations, successful organizations employ abstract reasoning to navigate ambiguity, uncertainty, and resistance. For example, IBM's Global Business Services division utilizes systemic thinking frameworks to map interdependencies, visualize desired end states, and align stakeholders around shared objectives, ensuring smoother transitions and sustained performance improvement (S Snowden & Boone, 2007).\n",
      "\n",
      "Conclusion\n",
      "\n",
      "As evidenced throughout this discussion, abstract reasoning constitutes a powerful cognitive toolkit indispensable for success in contemporary business and management environments. Its multifaceted benefits span various aspects of organizational life, including decision-making, problem-solving, communication, learning, and adaptation. To thrive amidst mounting complexity and accelerating change, aspiring professionals must cultivate their abstract reasoning abilities, embracing the challenge of mastering this essential skillset.<|EOT|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_ds[3]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9de4742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 2326\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def _pack_dataset(examples, seq_len):\n",
    "    flat = list(chain.from_iterable(examples[\"input_ids\"]))\n",
    "    n_full  = len(flat) // seq_len\n",
    "    chunks  = [flat[i*seq_len:(i+1)*seq_len] for i in range(n_full)]\n",
    "\n",
    "    return {\"input_ids\": chunks}\n",
    "\n",
    "def pack_dataset(ds, seq_len):\n",
    "    return ds.map(\n",
    "        _pack_dataset, \n",
    "        batched=True, \n",
    "        batch_size=100_000, \n",
    "        remove_columns=ds.column_names, \n",
    "        num_proc=128,\n",
    "        fn_kwargs={'seq_len': seq_len}\n",
    "    )\n",
    "\n",
    "packed_ds = pack_dataset(tokenized_ds, 4096)\n",
    "packed_ds.set_format('torch')\n",
    "print(packed_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e1cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import PreTrainedTokenizer\n",
    "import torch\n",
    "\n",
    "K = 1024\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForCausalLM:\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids = [instance[\"input_ids\"][:4*K] for instance in instances]\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8499181a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[125030,  11228,  22576,  ...,   1358,    263,  12536],\n",
       "         [   285,    514,   1700,  ...,     13, 125000,   3430],\n",
       "         [  1238,   7625,   8660,  ...,     14,     17,    771],\n",
       "         [  2299,   3697,  74836,  ...,  10571,   4543,   4297]]),\n",
       " 'labels': tensor([[125030,  11228,  22576,  ...,   1358,    263,  12536],\n",
       "         [   285,    514,   1700,  ...,     13, 125000,   3430],\n",
       "         [  1238,   7625,   8660,  ...,     14,     17,    771],\n",
       "         [  2299,   3697,  74836,  ...,  10571,   4543,   4297]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = DataCollatorForCausalLM(tokenizer)\n",
    "data_loader = DataLoader(packed_ds, collate_fn=collator, batch_size=4)\n",
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b52758",
   "metadata": {},
   "source": [
    "# 2. Build Intra-document Attention Mask (Using Flex-attention)\n",
    "\n",
    "![attention_mask.png](./attachment/attention_mask.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "823304d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlockMask(shape=(1, 1, 4096, 4096), sparsity=87.40%, \n",
      "(0, 0)\n",
      "â–‘â–‘                              \n",
      "â–ˆâ–ˆâ–‘â–‘                            \n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘                          \n",
      "â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘                        \n",
      "      â–ˆâ–ˆâ–‘â–‘                      \n",
      "      â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘                    \n",
      "          â–‘â–‘â–‘â–‘                  \n",
      "            â–‘â–‘â–‘â–‘                \n",
      "            â–‘â–‘â–ˆâ–ˆâ–‘â–‘              \n",
      "            â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘            \n",
      "            â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘          \n",
      "                    â–ˆâ–ˆâ–‘â–‘        \n",
      "                    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘      \n",
      "                    â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘    \n",
      "                          â–‘â–‘â–‘â–‘  \n",
      "                            â–ˆâ–ˆâ–‘â–‘\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.attention.flex_attention import create_block_mask, and_masks\n",
    "import torch\n",
    "input_ids = packed_ds[0]['input_ids']\n",
    "input_ids_2d = input_ids.unsqueeze(0).to('cuda')\n",
    "\n",
    "def _intra_doc_mask(input_ids, bos_token_id):\n",
    "    is_bos = (input_ids == bos_token_id)\n",
    "    is_bos_flat = is_bos.flatten()\n",
    "    flat_doc_ids = torch.cumsum(is_bos_flat.long(), 0)\n",
    "    doc_ids = flat_doc_ids.view_as(input_ids)\n",
    "    \n",
    "    def intra_doc_mask(b, h, q_idx, kv_idx):\n",
    "        same_doc = doc_ids[b, q_idx] == doc_ids[b, kv_idx]\n",
    "        return same_doc\n",
    "    \n",
    "    def causal_mask(b, h, q_idx, kv_idx):\n",
    "        return q_idx >= kv_idx\n",
    "\n",
    "    return and_masks(intra_doc_mask, causal_mask)\n",
    "\n",
    "def create_intra_doc_mask(input_ids, tokenizer):\n",
    "    model_bos_token_id = tokenizer.bos_token_id\n",
    "    B, Q_LEN = input_ids.shape\n",
    "    H = None \n",
    "    KV_LEN = Q_LEN\n",
    "\n",
    "    mask_mod_func = _intra_doc_mask(input_ids.to('cuda'), model_bos_token_id)\n",
    "\n",
    "    block_mask = create_block_mask(\n",
    "        mask_mod=mask_mod_func,\n",
    "        B=B,\n",
    "        H=H,\n",
    "        Q_LEN=Q_LEN,\n",
    "        KV_LEN=KV_LEN\n",
    "    )\n",
    "    return block_mask\n",
    "\n",
    "print(create_intra_doc_mask(input_ids_2d, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aea3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorIntraDocMask:\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids = [instance[\"input_ids\"][:4*K] for instance in instances]\n",
    "        input_ids = pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        block_mask = create_intra_doc_mask(input_ids, self.tokenizer)\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=block_mask,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45d9aab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualize Attention Masks\n",
      " BlockMask(shape=(2, 1, 4096, 4096), sparsity=86.23%, \n",
      "(0, 0)\n",
      "â–‘â–‘                              \n",
      "â–ˆâ–ˆâ–‘â–‘                            \n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘                          \n",
      "â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘                        \n",
      "      â–ˆâ–ˆâ–‘â–‘                      \n",
      "      â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘                    \n",
      "          â–‘â–‘â–‘â–‘                  \n",
      "            â–‘â–‘â–‘â–‘                \n",
      "            â–‘â–‘â–ˆâ–ˆâ–‘â–‘              \n",
      "            â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘            \n",
      "            â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘          \n",
      "                    â–ˆâ–ˆâ–‘â–‘        \n",
      "                    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘      \n",
      "                    â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘    \n",
      "                          â–‘â–‘â–‘â–‘  \n",
      "                            â–ˆâ–ˆâ–‘â–‘\n",
      "\n",
      "(1, 0)\n",
      "â–‘â–‘                              \n",
      "â–ˆâ–ˆâ–‘â–‘                            \n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘                          \n",
      "    â–‘â–‘â–‘â–‘                        \n",
      "    â–‘â–‘â–ˆâ–ˆâ–‘â–‘                      \n",
      "    â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘                    \n",
      "    â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘                  \n",
      "    â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘                \n",
      "              â–‘â–‘â–‘â–‘              \n",
      "              â–‘â–‘â–ˆâ–ˆâ–‘â–‘            \n",
      "              â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘          \n",
      "                    â–ˆâ–ˆâ–‘â–‘        \n",
      "                    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘      \n",
      "                    â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘    \n",
      "                          â–ˆâ–ˆâ–‘â–‘  \n",
      "                          â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorIntraDocMask(tokenizer)\n",
    "data_loader = DataLoader(packed_ds, collate_fn=collator, batch_size=2)\n",
    "batch = next(iter(data_loader))\n",
    "print(\"Visualize Attention Masks\\n\", batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60b6b0",
   "metadata": {},
   "source": [
    "# 3. Pretraining Setup with KORMoTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d5e270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention implementation:  flex_attention\n"
     ]
    }
   ],
   "source": [
    "from kormo.train.arguments import KORMoTrainingArguments\n",
    "from kormo.train.trainer import KORMoTrainer\n",
    "from kormo.modeling_configs.load_model import load_model_from_config\n",
    "\n",
    "model, _ = load_model_from_config('1B', _attn_implementation='flex_attention')\n",
    "model.to('cuda')\n",
    "print(\"Attention implementation: \", model.config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "712bd9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = KORMoTrainingArguments(\n",
    "    output_dir='./kormo-1B-PT',\n",
    "    per_device_train_batch_size=4,\n",
    "    lr_scheduler_type='linear',\n",
    "    logging_steps=10,\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "trainer = KORMoTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=packed_ds,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorIntraDocMask(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe2fa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 125040, 'pad_token_id': 125032}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchoics2623\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/raid/MLP/mjkim/KORMo-tutorial/tutorial/wandb/run-20251013_140901-dile0zie</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/choics2623/huggingface/runs/dile0zie' target=\"_blank\">prime-universe-199</a></strong> to <a href='https://wandb.ai/choics2623/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/choics2623/huggingface' target=\"_blank\">https://wandb.ai/choics2623/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/choics2623/huggingface/runs/dile0zie' target=\"_blank\">https://wandb.ai/choics2623/huggingface/runs/dile0zie</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='582' max='582' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [582/582 03:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.740500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>6.820800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>6.366900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>6.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>6.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>6.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>5.794300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>5.720800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.762500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>5.621400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>5.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>5.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>5.459100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.495100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>5.372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>5.401800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>5.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>5.314800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>5.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>5.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>5.167600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>5.229300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>5.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>5.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>5.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>4.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>5.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>4.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>4.884700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>4.984700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>4.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.836400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>4.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>4.880800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>4.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>4.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.816700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>4.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>4.897500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>4.889400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>4.738700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.771400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>4.777700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>4.714300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>4.746500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=582, training_loss=5.700584742621458, metrics={'train_runtime': 232.0052, 'train_samples_per_second': 10.026, 'train_steps_per_second': 2.509, 'total_flos': 6.069368398440038e+16, 'train_loss': 5.700584742621458, 'mean_token_accuracy': 0.2748473882675171, 'num_input_tokens_seen': '9.53M', 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_kormo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
