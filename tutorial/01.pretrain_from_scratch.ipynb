{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276af8b0",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/MLP-Lab/KORMo-tutorial.git\n",
    "!cd KORMo-tutorial & bash setup/create_uv_venv.sh\n",
    "!source .venv_kormo/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c15283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d94be",
   "metadata": {},
   "source": [
    "# 1. Preparing the Dataset for Pretraining\n",
    "\n",
    "This section builds the dataset pipeline for language model pretraining.\n",
    "\n",
    "- 1. Load the raw dataset\n",
    "- 2. Tokenize the text\n",
    "- 3. Pack sequences\n",
    "- 4. Data collator\n",
    "- 5. Verify the processed data with a DataLoader\n",
    "\n",
    "### Raw text $\\to$ Tokenize $\\to$ Sequence packing $\\to$ Data Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset from 'KORMo-Team/KORMo-tutorial-datasets'.\n",
    "import datasets \n",
    "\n",
    "dataset_repo_id = 'KORMo-Team/KORMo-tutorial-datasets'\n",
    "\n",
    "pt_dataset = datasets.load_dataset(\n",
    "    dataset_repo_id, \n",
    "    name='pretrain', \n",
    "    split='train'\n",
    ")\n",
    "\n",
    "train_ds = pt_dataset.shuffle(seed=42)\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "tokenizer_repo_id = 'KORMo-Team/KORMo-tokenizer'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1124561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all dataset\n",
    "\n",
    "def _tokenize(examples, tokenizer):\n",
    "    input_ids = []\n",
    "    for text in examples['text']:\n",
    "        input_ids.append(tokenizer.encode(text) + [tokenizer.eos_token_id])\n",
    "    return{\n",
    "        'input_ids': input_ids\n",
    "    }\n",
    "\n",
    "tokenized_ds = train_ds.map(\n",
    "    _tokenize, \n",
    "    batched=True, \n",
    "    num_proc=48,\n",
    "    remove_columns=train_ds.column_names,\n",
    "    fn_kwargs={'tokenizer': tokenizer},\n",
    ")\n",
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenized_ds[3]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def _pack_dataset(examples, seq_len):\n",
    "    flat = list(chain.from_iterable(examples[\"input_ids\"]))\n",
    "    n_full  = len(flat) // seq_len\n",
    "    chunks  = [flat[i*seq_len:(i+1)*seq_len] for i in range(n_full)]\n",
    "\n",
    "    return {\"input_ids\": chunks}\n",
    "\n",
    "def pack_dataset(ds, seq_len):\n",
    "    return ds.map(\n",
    "        _pack_dataset, \n",
    "        batched=True, \n",
    "        batch_size=100_000, \n",
    "        remove_columns=ds.column_names, \n",
    "        num_proc=128,\n",
    "        fn_kwargs={'seq_len': seq_len}\n",
    "    )\n",
    "\n",
    "packed_ds = pack_dataset(tokenized_ds, 4096)\n",
    "packed_ds.set_format('torch')\n",
    "print(packed_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e1cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import PreTrainedTokenizer\n",
    "import torch\n",
    "\n",
    "K = 1024\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForCausalLM:\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids = [instance[\"input_ids\"][:4*K] for instance in instances]\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = DataCollatorForCausalLM(tokenizer)\n",
    "data_loader = DataLoader(packed_ds, collate_fn=collator, batch_size=4)\n",
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b52758",
   "metadata": {},
   "source": [
    "# 2. Build Intra-document Attention Mask (Using Flex-attention)\n",
    "\n",
    "![attention_mask.png](./attachment/attention_mask.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823304d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention.flex_attention import create_block_mask, and_masks\n",
    "import torch\n",
    "input_ids = packed_ds[0]['input_ids']\n",
    "input_ids_2d = input_ids.unsqueeze(0).to('cuda')\n",
    "\n",
    "def _intra_doc_mask(input_ids, bos_token_id):\n",
    "    is_bos = (input_ids == bos_token_id)\n",
    "    is_bos_flat = is_bos.flatten()\n",
    "    flat_doc_ids = torch.cumsum(is_bos_flat.long(), 0)\n",
    "    doc_ids = flat_doc_ids.view_as(input_ids)\n",
    "    \n",
    "    def intra_doc_mask(b, h, q_idx, kv_idx):\n",
    "        same_doc = doc_ids[b, q_idx] == doc_ids[b, kv_idx]\n",
    "        return same_doc\n",
    "    \n",
    "    def causal_mask(b, h, q_idx, kv_idx):\n",
    "        return q_idx >= kv_idx\n",
    "\n",
    "    return and_masks(intra_doc_mask, causal_mask)\n",
    "\n",
    "def create_intra_doc_mask(input_ids, tokenizer):\n",
    "    model_bos_token_id = tokenizer.bos_token_id\n",
    "    B, Q_LEN = input_ids.shape\n",
    "    H = None \n",
    "    KV_LEN = Q_LEN\n",
    "\n",
    "    mask_mod_func = _intra_doc_mask(input_ids.to('cuda'), model_bos_token_id)\n",
    "\n",
    "    block_mask = create_block_mask(\n",
    "        mask_mod=mask_mod_func,\n",
    "        B=B,\n",
    "        H=H,\n",
    "        Q_LEN=Q_LEN,\n",
    "        KV_LEN=KV_LEN\n",
    "    )\n",
    "    return block_mask\n",
    "\n",
    "print(create_intra_doc_mask(input_ids_2d, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorIntraDocMask:\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids = [instance[\"input_ids\"][:4*K] for instance in instances]\n",
    "        input_ids = pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        block_mask = create_intra_doc_mask(input_ids, self.tokenizer)\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=block_mask,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorIntraDocMask(tokenizer)\n",
    "data_loader = DataLoader(packed_ds, collate_fn=collator, batch_size=2)\n",
    "batch = next(iter(data_loader))\n",
    "print(\"Visualize Attention Masks\\n\", batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60b6b0",
   "metadata": {},
   "source": [
    "# 3. Pretraining Setup with KORMoTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kormo.train.arguments import KORMoTrainingArguments\n",
    "from kormo.train.trainer import KORMoTrainer\n",
    "from kormo.modeling_configs.load_model import load_model_from_config\n",
    "\n",
    "model, _ = load_model_from_config('1B', _attn_implementation='flex_attention')\n",
    "model.to('cuda')\n",
    "print(\"Attention implementation: \", model.config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712bd9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = KORMoTrainingArguments(\n",
    "    output_dir='./kormo-1B-PT',\n",
    "    per_device_train_batch_size=4,\n",
    "    lr_scheduler_type='linear',\n",
    "    logging_steps=10,\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "trainer = KORMoTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=packed_ds,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorIntraDocMask(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_kormo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
