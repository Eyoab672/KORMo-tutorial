{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c15283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d94be",
   "metadata": {},
   "source": [
    "# 1. Preparing the Dataset for Pretraining\n",
    "\n",
    "This section builds the dataset pipeline for language model pretraining.\n",
    "\n",
    "- 1. Load the raw dataset\n",
    "- 2. Tokenize the text\n",
    "- 3. Pack sequences\n",
    "- 4. Data collator\n",
    "- 5. Verify the processed data with a DataLoader\n",
    "\n",
    "### Raw text $\\to$ Tokenize $\\to$ Sequence packing $\\to$ Data Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load \"cosmopedia_auto_math_text\"...\n",
      "Load \"cosmopedia_khanacademy\"...\n",
      "Load \"cosmopedia_openstax\"...\n",
      "Load \"cosmopedia_stanford\"...\n",
      "Load \"cosmopedia_stories\"...\n",
      "Load \"cosmopedia_web_samples_v1\"...\n",
      "Load \"cosmopedia_web_samples_v2\"...\n",
      "Load \"cosmopedia_wikihow\"...\n"
     ]
    }
   ],
   "source": [
    "# Load the raw dataset from 'KORMo-Team/KORMo-tutorial-datasets'.\n",
    "import datasets \n",
    "\n",
    "dataset_repo_id = 'KORMo-Team/KORMo-tutorial-datasets'\n",
    "config_names = datasets.get_dataset_config_names(dataset_repo_id)\n",
    "\n",
    "dataset = []\n",
    "for name in config_names:\n",
    "    print(f\"Load \\\"{name}\\\"...\")\n",
    "    text_dataset = datasets.load_dataset(dataset_repo_id, name=name, split='train').select_columns(['text'])\n",
    "    dataset.append(text_dataset)\n",
    "\n",
    "train_ds = datasets.concatenate_datasets(dataset)\n",
    "train_ds = train_ds.shuffle(seed=42)\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150b4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "tokenizer_repo_id = 'KORMo-Team/KORMo-tokenizer'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1124561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 8000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all dataset\n",
    "\n",
    "def _tokenize(examples, tokenizer):\n",
    "    input_ids = []\n",
    "    for text in examples['text']:\n",
    "        input_ids.append(tokenizer.encode(text) + [tokenizer.eos_token_id])\n",
    "    return{\n",
    "        'input_ids': input_ids\n",
    "    }\n",
    "\n",
    "tokenized_ds = train_ds.map(\n",
    "    _tokenize, \n",
    "    batched=True, \n",
    "    num_proc=48,\n",
    "    remove_columns=train_ds.column_names,\n",
    "    fn_kwargs={'tokenizer': tokenizer},\n",
    ")\n",
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97bf5c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|BOS|> **The Multiplier Effect in the Keynesian Cross Model:**\n",
      "\n",
      "The Keynesian perspective on macroeconomic theory posits that government spending can serve as a crucial tool for managing economic fluctuations and promoting full employment. However, the relationship between changes in government spending and their impact on output is nuanced and complex due to the presence of the multiplier effect. This phenomenon suggests that a given increase in government spending may lead to a larger overall shift in equilibrium national income.\n",
      "\n",
      "To illustrate, consider an economy where the intersection of the aggregate expenditure function and the 45-degree line occurs at a GDP of $700, whereas the level of potential GDP equals $800. At first glance, it may appear logical to assume that increasing government spending by $100 would suffice to reach potential GDP. Nevertheless, such reasoning overlooks the intricate interplay of various components within the model, specifically the induced increases in consumption expenditure triggered by higher levels of national income.\n",
      "\n",
      "Formally, let C(Y) represent consumption as a function of national income Y, I denote investment, G stand for government spending, X symbolize exports, and IM import demand. Total planned expenditure E, then, can be expressed as follows:\n",
      "\n",
      "E=C(Y)+I+G+X−IM(Y)\\begin{aligned}\n",
      "&E = C(Y) + I + G + X - IM(Y)\n",
      "\\end{aligned}E=C(Y)+I+G+X−IM(Y)\n",
      "\n",
      "Here, we focus on the marginal propensity to consume, denoted MPC, which signifies the proportion of each additional dollar earned that gets spent on consumption goods and services. For instance, if the MPC amounts to 0.6, every extra dollar received translates into $0.6 worth of added consumer spending. Consequently, when autonomous government spending rises by $\\Delta G$, the ensuing rise in equilibrium GDP ($\\Delta Y$) depends not only on the initial injection but also on the subsequent chain reaction through increased consumption. Mathematically, this process adheres to the formula below:\n",
      "\n",
      "ΔY=(1−MPC)−1×ΔG\\begin{aligned}\n",
      "&\\Delta Y = \\frac{(1-\\text{MPC})}{1}\\times\\Delta G \\\\\n",
      "\\Rightarrow &\\Delta Y=\\frac{1}{\\left(1-\\text{MPC}\\right)}\\times\\Delta G\n",
      "\\end{aligned}ΔY=1−MPC1​×ΔG⇒ΔY=1−MPC1​×ΔG\n",
      "\n",
      "This expression encapsulates the essence of the multiplier effect—an incremental boost in government outlays generates amplified repercussions throughout the entire system. Specifically, the magnitude of these ramifications hinges upon the value of $(1-(1-\\text{MPC}))^{-1}$, known as the multiplier. To elaborate further, assuming an MPC equal to 0.6 implies that an expansionary fiscal policy intervention engenders a multiplied response:\n",
      "\n",
      "Multiplier=(1−0.6)−1≈2.5\\begin{aligned}\n",
      "\\text{Multiplier}&=\\frac{1}{(1-0.6)} \\approx 2.5\n",
      "\\end{aligned}Multiplier=1−0.61​≈2.5\n",
      "\n",
      "Consequently, raising government spending by $100 ultimately culminates in a $250 surge in GDP ($100 / (1-0.6)$). Thus, policymakers must account for the potency of the multiplier effect when devising strategies aimed at achieving specific macroeconomic objectives.\n",
      "\n",
      "Empirical assessments corroborating the existence and significance of the multiplier effect abound across diverse contexts and economies. Nonetheless, its precise quantification remains elusive owing to factors like varying degrees of openness, differing magnitudes of wealth effects, and heterogeneous financial systems among countries under investigation. Furthermore, recent studies emphasize the importance of considering the dynamic nature of the multiplier over time, thereby underscoring the necessity of refining our understanding of its workings beyond static models.\n",
      "\n",
      "In conclusion, the multiplier effect constitutes a vital aspect of the Keynesian cross model, significantly influencing the connection between alterations in public spending and corresponding shifts in equilibrium output. As demonstrated above, comprehending this mechanism enables practitioners and scholars alike to better appreciate both the theoretical underpinnings and practical implications of activist fiscal policies designed to stabilize business cycles and foster sustainable growth trajectories.<|EOT|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_ds[3]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def _pack_dataset(examples, seq_len):\n",
    "    flat = list(chain.from_iterable(examples[\"input_ids\"]))\n",
    "    n_full  = len(flat) // seq_len\n",
    "    chunks  = [flat[i*seq_len:(i+1)*seq_len] for i in range(n_full)]\n",
    "\n",
    "    return {\"input_ids\": chunks}\n",
    "\n",
    "def pack_dataset(ds, seq_len):\n",
    "    return ds.map(\n",
    "        _pack_dataset, \n",
    "        batched=True, \n",
    "        batch_size=100_000, \n",
    "        remove_columns=ds.column_names, \n",
    "        num_proc=128,\n",
    "        fn_kwargs={'seq_len': seq_len}\n",
    "    )\n",
    "\n",
    "packed_ds = pack_dataset(tokenized_ds, 4096)\n",
    "packed_ds.set_format('torch')\n",
    "print(packed_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e1cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import PreTrainedTokenizer\n",
    "import torch\n",
    "\n",
    "K = 1024\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForCausalLM:\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids = [instance[\"input_ids\"][:4*K] for instance in instances]\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499181a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[125030,  16627,   2312,  ...,   2094,    626,    601],\n",
       "         [  9017,     13, 125000,  ...,    269,  12447,    960],\n",
       "         [  1870,    285,    534,  ...,    626,   2889,    281],\n",
       "         [   401,  10490,    437,  ...,     13,    832,   9333]]),\n",
       " 'labels': tensor([[125030,  16627,   2312,  ...,   2094,    626,    601],\n",
       "         [  9017,     13, 125000,  ...,    269,  12447,    960],\n",
       "         [  1870,    285,    534,  ...,    626,   2889,    281],\n",
       "         [   401,  10490,    437,  ...,     13,    832,   9333]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = DataCollatorForCausalLM(tokenizer)\n",
    "data_loader = DataLoader(packed_ds, collate_fn=collator, batch_size=4)\n",
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b52758",
   "metadata": {},
   "source": [
    "# 2. Build Intra-document Attention Mask (Using Flex-attention)\n",
    "\n",
    "![attention_mask.png](./attachment/attention_mask.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823304d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlockMask(shape=(1, 1, 4096, 4096), sparsity=84.96%, \n",
      "(0, 0)\n",
      "░░                              \n",
      "██░░                            \n",
      "████░░                          \n",
      "    ░░░░                        \n",
      "    ░░██░░                      \n",
      "    ░░████░░                    \n",
      "    ░░██████░░                  \n",
      "    ░░░░░░░░░░░░                \n",
      "              ██░░              \n",
      "              ████░░            \n",
      "              ██████░░          \n",
      "                    ░░░░        \n",
      "                    ░░██░░      \n",
      "                    ░░████░░    \n",
      "                    ░░░░░░░░░░  \n",
      "                            ██░░\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.attention.flex_attention import create_block_mask, and_masks\n",
    "import torch\n",
    "input_ids = packed_ds[0]['input_ids']\n",
    "input_ids_2d = input_ids.unsqueeze(0).to('cuda')\n",
    "\n",
    "def _intra_doc_mask(input_ids, bos_token_id):\n",
    "    is_bos = (input_ids == bos_token_id)\n",
    "    is_bos_flat = is_bos.flatten()\n",
    "    flat_doc_ids = torch.cumsum(is_bos_flat.long(), 0)\n",
    "    doc_ids = flat_doc_ids.view_as(input_ids)\n",
    "    \n",
    "    def intra_doc_mask(b, h, q_idx, kv_idx):\n",
    "        same_doc = doc_ids[b, q_idx] == doc_ids[b, kv_idx]\n",
    "        return same_doc\n",
    "    \n",
    "    def causal_mask(b, h, q_idx, kv_idx):\n",
    "        return q_idx >= kv_idx\n",
    "\n",
    "    return and_masks(intra_doc_mask, causal_mask)\n",
    "\n",
    "def create_intra_doc_mask(input_ids, tokenizer):\n",
    "    model_bos_token_id = tokenizer.bos_token_id\n",
    "    B, Q_LEN = input_ids.shape\n",
    "    H = None \n",
    "    KV_LEN = Q_LEN\n",
    "\n",
    "    mask_mod_func = _intra_doc_mask(input_ids.to('cuda'), model_bos_token_id)\n",
    "\n",
    "    block_mask = create_block_mask(\n",
    "        mask_mod=mask_mod_func,\n",
    "        B=B,\n",
    "        H=H,\n",
    "        Q_LEN=Q_LEN,\n",
    "        KV_LEN=KV_LEN\n",
    "    )\n",
    "    return block_mask\n",
    "\n",
    "print(create_intra_doc_mask(input_ids_2d, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorIntraDocMask:\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids = [instance[\"input_ids\"][:4*K] for instance in instances]\n",
    "        input_ids = pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        block_mask = create_intra_doc_mask(input_ids, self.tokenizer)\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=block_mask,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9aab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualize Attention Masks BlockMask(shape=(2, 1, 4096, 4096), sparsity=84.96%, \n",
      "(0, 0)\n",
      "░░                              \n",
      "██░░                            \n",
      "████░░                          \n",
      "    ░░░░                        \n",
      "    ░░██░░                      \n",
      "    ░░████░░                    \n",
      "    ░░██████░░                  \n",
      "    ░░░░░░░░░░░░                \n",
      "              ██░░              \n",
      "              ████░░            \n",
      "              ██████░░          \n",
      "                    ░░░░        \n",
      "                    ░░██░░      \n",
      "                    ░░████░░    \n",
      "                    ░░░░░░░░░░  \n",
      "                            ██░░\n",
      "\n",
      "(1, 0)\n",
      "░░                              \n",
      "██░░                            \n",
      "████░░                          \n",
      "░░░░░░░░                        \n",
      "      ░░░░                      \n",
      "        ██░░                    \n",
      "        ████░░                  \n",
      "        ░░░░░░░░                \n",
      "              ██░░              \n",
      "              ████░░            \n",
      "              ██████░░          \n",
      "              ████████░░        \n",
      "              ░░░░░░░░░░░░      \n",
      "                        ██░░    \n",
      "                        ████░░  \n",
      "                        ░░░░░░░░\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorIntraDocMask(tokenizer)\n",
    "data_loader = DataLoader(packed_ds, collate_fn=collator, batch_size=2)\n",
    "batch = next(iter(data_loader))\n",
    "print(\"Visualize Attention Masks\\n\", batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60b6b0",
   "metadata": {},
   "source": [
    "# 3. Pretraining Setup with KORMoTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kormo.train.arguments import KORMoTrainingArguments\n",
    "from kormo.train.trainer import KORMoTrainer\n",
    "from kormo.modeling_configs.load_model import load_model_from_config\n",
    "\n",
    "model, _ = load_model_from_config('1B', _attn_implementation='flex_attention')\n",
    "model.to('cuda')\n",
    "print(\"Attention implementation: \", model.config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712bd9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = KORMoTrainingArguments(\n",
    "    output_dir='./kormo-1B-PT',\n",
    "    per_device_train_batch_size=4,\n",
    "    lr_scheduler_type='linear',\n",
    "    logging_steps=10,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = KORMoTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=packed_ds,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorIntraDocMask(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fe2fa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 125040, 'pad_token_id': 125032}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='338' max='338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [338/338 02:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>6.970300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>6.503200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.507900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>6.382400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>6.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>6.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>6.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>6.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>5.966900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>5.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>5.761600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>5.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>5.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.631400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>5.658100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>5.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>5.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>5.573300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>5.462100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>5.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>5.522300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=338, training_loss=6.432804906156641, metrics={'train_runtime': 136.8114, 'train_samples_per_second': 9.882, 'train_steps_per_second': 2.471, 'total_flos': 3.527852998577357e+16, 'train_loss': 6.432804906156641, 'mean_token_accuracy': 0.22595390863716602, 'num_input_tokens_seen': '5.54M', 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_kormo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
